package judge

import (
	"context"
	"encoding/json"
	"fmt"
	"regexp"
	"strings"
)

// SkillScores holds the LLM judge scores for a SKILL.md file.
type SkillScores struct {
	Clarity            int     `json:"clarity"`
	Actionability      int     `json:"actionability"`
	TokenEfficiency    int     `json:"token_efficiency"`
	ScopeDiscipline    int     `json:"scope_discipline"`
	DirectivePrecision int     `json:"directive_precision"`
	Novelty            int     `json:"novelty"`
	Overall            float64 `json:"overall"`
	BriefAssessment    string  `json:"brief_assessment"`
}

// RefScores holds the LLM judge scores for a reference file.
type RefScores struct {
	Clarity            int     `json:"clarity"`
	InstructionalValue int     `json:"instructional_value"`
	TokenEfficiency    int     `json:"token_efficiency"`
	Novelty            int     `json:"novelty"`
	SkillRelevance     int     `json:"skill_relevance"`
	Overall            float64 `json:"overall"`
	BriefAssessment    string  `json:"brief_assessment"`
}

var skillDims = []string{"clarity", "actionability", "token_efficiency", "scope_discipline", "directive_precision", "novelty"}
var refDims = []string{"clarity", "instructional_value", "token_efficiency", "novelty", "skill_relevance"}

// SkillDimensions returns the dimension names for SKILL.md scoring.
func SkillDimensions() []string { return skillDims }

// RefDimensions returns the dimension names for reference file scoring.
func RefDimensions() []string { return refDims }

// ---------------------------------------------------------------------------
// Judge prompts — ported from analysis/llm_judge.py
// ---------------------------------------------------------------------------

const skillJudgePrompt = `You are evaluating the quality of an "Agent Skill" — a markdown document that instructs an AI coding agent how to perform a specific task.  Score this skill on 6 dimensions, each from 1 (worst) to 5 (best). Use the full range — reserve 5 for genuinely excellent output and do not round up:

**Scoring dimensions:**

1. **Clarity** (1-5): How clear and unambiguous are the instructions? Are there vague or confusing passages?
   - 1: Mostly vague, unclear instructions; an agent would frequently misinterpret intent
   - 2: Several unclear passages that would cause an agent to guess or ask for clarification
   - 3: Generally clear with some ambiguities; an agent could follow most instructions but would stumble on a few
   - 4: Clear throughout with only minor phrasing that could be tightened; an agent would rarely misinterpret
   - 5: Crystal clear, no room for misinterpretation; every instruction has exactly one reading

2. **Actionability** (1-5): How actionable are the instructions for an AI agent? Can an agent follow them step-by-step?
   - 1: Abstract advice, no concrete steps; an agent could not act on these instructions
   - 2: Mostly abstract with a few concrete steps scattered throughout
   - 3: Mix of concrete and abstract guidance; an agent could act on roughly half the content directly
   - 4: Mostly concrete and actionable with occasional abstract guidance that lacks specific steps
   - 5: Highly specific, step-by-step instructions an agent can execute without interpretation

3. **Token Efficiency** (1-5): How concise is the skill? Does every token earn its place in the context window, or is there redundant prose, boilerplate, or filler that could be trimmed without losing instructional value?
   - 1: Extremely verbose, heavy boilerplate; could cut 50%+ without losing instructional value
   - 2: Notably verbose; significant sections of redundant explanation, filler, or repeated content that could be cut
   - 3: Reasonably concise with some unnecessary verbosity; ~20-30% could be trimmed
   - 4: Concise with only minor redundancies; nearly every paragraph earns its place
   - 5: Maximally concise — every sentence carries essential information; nothing to cut

4. **Scope Discipline** (1-5): Does the skill stay tightly focused on its stated purpose and primary language/technology, or does it sprawl into adjacent domains, languages, or concerns that risk confusing the agent?
   - 1: Sprawling scope, mixes many unrelated languages or domains; unclear what the skill is actually for
   - 2: Covers its primary purpose but includes substantial tangential content in other languages or domains
   - 3: Mostly focused with some tangential content that an agent might incorrectly apply to the wrong context
   - 4: Well-focused on its purpose with only brief mentions of adjacent concerns that are clearly delineated
   - 5: Tightly scoped to a single purpose and technology; no content an agent could misapply

5. **Directive Precision** (1-5): Does the skill use precise, unambiguous directives (must, always, never, ensure) or does it hedge with vague suggestions (consider, may, could, possibly)?
   - 1: Mostly vague suggestions and hedged language; an agent would not know what is required vs. optional
   - 2: More hedging than precision; important instructions are often phrased as suggestions
   - 3: Mix of precise directives and vague guidance; critical steps are usually precise but supporting guidance hedges
   - 4: Mostly precise directives with occasional hedging on less critical points
   - 5: Consistently precise, imperative directives throughout; every instruction is unambiguous about whether it is required

6. **Novelty** (1-5): How much of this skill's content provides information beyond what you would already know from training data? Does it convey project-specific conventions, proprietary APIs, internal workflows, or non-obvious domain knowledge — or does it mostly restate common programming knowledge you already have?
   - 1: Almost entirely common knowledge any LLM would already know; standard library docs, basic patterns, introductory tutorials
   - 2: Mostly common knowledge with a few pieces of genuinely new information (e.g., a specific version pin, one non-obvious convention) embedded in otherwise familiar content
   - 3: Roughly equal mix of common knowledge and genuinely new information; the novel parts are useful but interspersed with content you already know well
   - 4: Majority novel information — proprietary APIs, internal conventions, non-obvious gotchas — with some standard knowledge included for context or completeness
   - 5: Predominantly novel; nearly every section provides information not available in training data (proprietary systems, unpublished APIs, organization-specific workflows)

Respond with ONLY a JSON object in this exact format:
{
  "clarity": <1-5>,
  "actionability": <1-5>,
  "token_efficiency": <1-5>,
  "scope_discipline": <1-5>,
  "directive_precision": <1-5>,
  "novelty": <1-5>,
  "brief_assessment": "<1-2 sentence summary>"
}`

const refJudgePromptTemplate = `You are evaluating the quality of a **reference file** that accompanies an Agent Skill. Reference files are supplementary documents (examples, API docs, patterns, etc.) loaded alongside the main SKILL.md into an AI coding agent's context window.

The parent skill's purpose is provided below so you can judge whether this reference supports it.

**Parent skill:** %s
**Parent description:** %s

Score this reference file on 5 dimensions, each from 1 (worst) to 5 (best). Use the full range — reserve 5 for genuinely excellent output and do not round up:

**Scoring dimensions:**

1. **Clarity** (1-5): How clear and well-written is this reference? Can an AI agent easily parse and apply the information?
   - 1: Confusing, poorly formatted, hard to extract useful information
   - 2: Partially readable but disorganized; an agent would need to work to extract key information
   - 3: Generally clear with some ambiguities or formatting issues; usable but not optimized for agent consumption
   - 4: Well-structured and clear with only minor formatting or organizational issues
   - 5: Crystal clear, well-structured, easy for an agent to consume; information hierarchy is immediately apparent

2. **Instructional Value** (1-5): Does this reference provide concrete, directly-applicable examples, patterns, or API signatures that an agent can use — or is it abstract and theoretical?
   - 1: Abstract descriptions with no concrete examples or patterns; an agent cannot act on this content
   - 2: Mostly abstract with a few concrete examples that are insufficient for practical use
   - 3: Mix of concrete examples and abstract explanations; an agent could use some content directly but would need to fill gaps
   - 4: Mostly concrete and directly applicable with occasional abstract sections that lack working examples
   - 5: Rich with directly-applicable code examples, patterns, and signatures; an agent could use the content as-is

3. **Token Efficiency** (1-5): Does every token in this reference earn its place in the context window? Is the content concise, or bloated with redundant explanations, excessive boilerplate, or content that could be significantly compressed?
   - 1: Extremely verbose; could cut 50%%+ without losing useful information
   - 2: Notably verbose; significant redundancy, repeated explanations, or boilerplate that inflates token count
   - 3: Reasonably concise with some unnecessary verbosity; ~20-30%% could be trimmed
   - 4: Concise with only minor redundancies; nearly every section earns its token budget
   - 5: Maximally concise — every section carries essential information; nothing to cut

4. **Novelty** (1-5): How much of this reference provides information beyond what you would already know from training data? Does it document proprietary APIs, internal conventions, non-obvious gotchas, or uncommon patterns — or does it mostly restate standard documentation you already have access to?
   - 1: Almost entirely common knowledge (standard library docs, well-known patterns, basic tutorials)
   - 2: Mostly common knowledge with a few novel details (e.g., specific version requirements, one unusual configuration) embedded in otherwise familiar content
   - 3: Roughly equal mix of common knowledge and genuinely new information; the novel parts are useful but interspersed with familiar documentation
   - 4: Majority novel information — proprietary API details, internal conventions, non-obvious gotchas — with some standard content included for context
   - 5: Predominantly novel; nearly every section documents proprietary APIs, unpublished interfaces, or organization-specific patterns not in training data

5. **Skill Relevance** (1-5): How directly does this reference file support the parent skill's stated purpose? Does every section contribute to what the skill is trying to teach the agent, or does it include tangential content?
   - 1: Mostly unrelated to the parent skill's purpose; appears to be a generic reference bundled without curation
   - 2: Partially relevant but includes substantial tangential content unrelated to the skill's stated purpose
   - 3: Generally relevant with some tangential sections that an agent would need to filter out
   - 4: Clearly relevant to the skill's purpose with only minor tangential content
   - 5: Every section directly supports the parent skill's purpose; tightly curated for the skill's specific use case

Respond with ONLY a JSON object in this exact format:
{
  "clarity": <1-5>,
  "instructional_value": <1-5>,
  "token_efficiency": <1-5>,
  "novelty": <1-5>,
  "skill_relevance": <1-5>,
  "brief_assessment": "<1-2 sentence summary>"
}`

// DefaultMaxContentLen is the default maximum content length sent to the judge (characters).
// Use 0 to disable truncation.
const DefaultMaxContentLen = 8000

// ScoreSkill sends a SKILL.md's content to the LLM judge and returns parsed scores.
// maxLen controls content truncation (0 = no truncation).
func ScoreSkill(ctx context.Context, content string, client LLMClient, maxLen int) (*SkillScores, error) {
	userContent := formatUserContent(content, maxLen)
	text, err := client.Complete(ctx, skillJudgePrompt, userContent)
	if err != nil {
		return nil, fmt.Errorf("scoring SKILL.md: %w", err)
	}

	scores, err := parseSkillScores(text)
	if err != nil {
		return nil, err
	}

	// Retry if dimensions are missing
	missing := missingSkillDims(scores)
	if len(missing) > 0 {
		retryPrompt := skillJudgePrompt + "\n\nIMPORTANT: Your response MUST include ALL dimensions. You MUST include these keys in your JSON: " + strings.Join(missing, ", ")
		text, err = client.Complete(ctx, retryPrompt, userContent)
		if err != nil {
			// Return partial scores rather than failing entirely
			scores.Overall = computeSkillOverall(scores)
			return scores, nil
		}
		retry, err := parseSkillScores(text)
		if err == nil {
			scores = mergeSkillScores(scores, retry)
		}
	}

	scores.Overall = computeSkillOverall(scores)
	return scores, nil
}

// ScoreReference sends a reference file's content to the LLM judge and returns parsed scores.
// maxLen controls content truncation (0 = no truncation).
func ScoreReference(ctx context.Context, content, skillName, skillDesc string, client LLMClient, maxLen int) (*RefScores, error) {
	if skillName == "" {
		skillName = "(unnamed skill)"
	}
	if skillDesc == "" {
		skillDesc = "(no description provided)"
	}

	systemPrompt := fmt.Sprintf(refJudgePromptTemplate, skillName, skillDesc)
	userContent := formatUserContent(content, maxLen)

	text, err := client.Complete(ctx, systemPrompt, userContent)
	if err != nil {
		return nil, fmt.Errorf("scoring reference file: %w", err)
	}

	scores, err := parseRefScores(text)
	if err != nil {
		return nil, err
	}

	// Retry if dimensions are missing
	missing := missingRefDims(scores)
	if len(missing) > 0 {
		retryPrompt := systemPrompt + "\n\nIMPORTANT: Your response MUST include ALL dimensions. You MUST include these keys in your JSON: " + strings.Join(missing, ", ")
		text, err = client.Complete(ctx, retryPrompt, userContent)
		if err != nil {
			scores.Overall = computeRefOverall(scores)
			return scores, nil
		}
		retry, err := parseRefScores(text)
		if err == nil {
			scores = mergeRefScores(scores, retry)
		}
	}

	scores.Overall = computeRefOverall(scores)
	return scores, nil
}

// AggregateRefScores computes mean scores across multiple reference file results.
func AggregateRefScores(results []*RefScores) *RefScores {
	if len(results) == 0 {
		return nil
	}

	agg := &RefScores{}
	for _, r := range results {
		agg.Clarity += r.Clarity
		agg.InstructionalValue += r.InstructionalValue
		agg.TokenEfficiency += r.TokenEfficiency
		agg.Novelty += r.Novelty
		agg.SkillRelevance += r.SkillRelevance
	}

	n := len(results)
	agg.Clarity = (agg.Clarity + n/2) / n
	agg.InstructionalValue = (agg.InstructionalValue + n/2) / n
	agg.TokenEfficiency = (agg.TokenEfficiency + n/2) / n
	agg.Novelty = (agg.Novelty + n/2) / n
	agg.SkillRelevance = (agg.SkillRelevance + n/2) / n
	agg.Overall = computeRefOverall(agg)
	return agg
}

// --- Internal helpers ---

func formatUserContent(content string, maxLen int) string {
	if maxLen > 0 && len(content) > maxLen {
		content = content[:maxLen]
	}
	return "CONTENT TO EVALUATE:\n\n" + content
}

var jsonObjectRe = regexp.MustCompile(`\{[^{}]+\}`)

// extractJSON finds the first JSON object in the response text.
func extractJSON(text string) (string, error) {
	text = strings.TrimSpace(text)
	if strings.HasPrefix(text, "{") {
		// Try to parse the whole thing first
		end := strings.LastIndex(text, "}")
		if end >= 0 {
			candidate := text[:end+1]
			if json.Valid([]byte(candidate)) {
				return candidate, nil
			}
		}
	}

	// Search for embedded JSON object
	match := jsonObjectRe.FindString(text)
	if match != "" && json.Valid([]byte(match)) {
		return match, nil
	}

	return "", fmt.Errorf("no valid JSON object found in response: %.100s", text)
}

func parseSkillScores(text string) (*SkillScores, error) {
	jsonStr, err := extractJSON(text)
	if err != nil {
		return nil, err
	}

	var scores SkillScores
	if err := json.Unmarshal([]byte(jsonStr), &scores); err != nil {
		return nil, fmt.Errorf("parsing skill scores: %w", err)
	}

	return &scores, nil
}

func parseRefScores(text string) (*RefScores, error) {
	jsonStr, err := extractJSON(text)
	if err != nil {
		return nil, err
	}

	var scores RefScores
	if err := json.Unmarshal([]byte(jsonStr), &scores); err != nil {
		return nil, fmt.Errorf("parsing reference scores: %w", err)
	}

	return &scores, nil
}

func missingSkillDims(s *SkillScores) []string {
	var missing []string
	if s.Clarity == 0 {
		missing = append(missing, "clarity")
	}
	if s.Actionability == 0 {
		missing = append(missing, "actionability")
	}
	if s.TokenEfficiency == 0 {
		missing = append(missing, "token_efficiency")
	}
	if s.ScopeDiscipline == 0 {
		missing = append(missing, "scope_discipline")
	}
	if s.DirectivePrecision == 0 {
		missing = append(missing, "directive_precision")
	}
	if s.Novelty == 0 {
		missing = append(missing, "novelty")
	}
	return missing
}

func missingRefDims(s *RefScores) []string {
	var missing []string
	if s.Clarity == 0 {
		missing = append(missing, "clarity")
	}
	if s.InstructionalValue == 0 {
		missing = append(missing, "instructional_value")
	}
	if s.TokenEfficiency == 0 {
		missing = append(missing, "token_efficiency")
	}
	if s.Novelty == 0 {
		missing = append(missing, "novelty")
	}
	if s.SkillRelevance == 0 {
		missing = append(missing, "skill_relevance")
	}
	return missing
}

func computeSkillOverall(s *SkillScores) float64 {
	vals := []int{s.Clarity, s.Actionability, s.TokenEfficiency, s.ScopeDiscipline, s.DirectivePrecision, s.Novelty}
	return computeMean(vals)
}

func computeRefOverall(s *RefScores) float64 {
	vals := []int{s.Clarity, s.InstructionalValue, s.TokenEfficiency, s.Novelty, s.SkillRelevance}
	return computeMean(vals)
}

func computeMean(vals []int) float64 {
	var sum int
	var count int
	for _, v := range vals {
		if v > 0 {
			sum += v
			count++
		}
	}
	if count == 0 {
		return 0
	}
	return float64(sum*100/count) / 100
}

func mergeSkillScores(base, retry *SkillScores) *SkillScores {
	// If retry is complete, prefer it
	if len(missingSkillDims(retry)) == 0 {
		retry.BriefAssessment = coalesce(retry.BriefAssessment, base.BriefAssessment)
		return retry
	}
	// Fill gaps from retry
	if base.Clarity == 0 && retry.Clarity != 0 {
		base.Clarity = retry.Clarity
	}
	if base.Actionability == 0 && retry.Actionability != 0 {
		base.Actionability = retry.Actionability
	}
	if base.TokenEfficiency == 0 && retry.TokenEfficiency != 0 {
		base.TokenEfficiency = retry.TokenEfficiency
	}
	if base.ScopeDiscipline == 0 && retry.ScopeDiscipline != 0 {
		base.ScopeDiscipline = retry.ScopeDiscipline
	}
	if base.DirectivePrecision == 0 && retry.DirectivePrecision != 0 {
		base.DirectivePrecision = retry.DirectivePrecision
	}
	if base.Novelty == 0 && retry.Novelty != 0 {
		base.Novelty = retry.Novelty
	}
	if base.BriefAssessment == "" {
		base.BriefAssessment = retry.BriefAssessment
	}
	return base
}

func mergeRefScores(base, retry *RefScores) *RefScores {
	if len(missingRefDims(retry)) == 0 {
		retry.BriefAssessment = coalesce(retry.BriefAssessment, base.BriefAssessment)
		return retry
	}
	if base.Clarity == 0 && retry.Clarity != 0 {
		base.Clarity = retry.Clarity
	}
	if base.InstructionalValue == 0 && retry.InstructionalValue != 0 {
		base.InstructionalValue = retry.InstructionalValue
	}
	if base.TokenEfficiency == 0 && retry.TokenEfficiency != 0 {
		base.TokenEfficiency = retry.TokenEfficiency
	}
	if base.Novelty == 0 && retry.Novelty != 0 {
		base.Novelty = retry.Novelty
	}
	if base.SkillRelevance == 0 && retry.SkillRelevance != 0 {
		base.SkillRelevance = retry.SkillRelevance
	}
	if base.BriefAssessment == "" {
		base.BriefAssessment = retry.BriefAssessment
	}
	return base
}

func coalesce(a, b string) string {
	if a != "" {
		return a
	}
	return b
}
